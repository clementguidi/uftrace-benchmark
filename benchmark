#!/usr/bin/env bash

set -euo pipefail

echo "# # # # # # # # # # # # # # # # # # #"
echo "# DORSAL's uftrace benchmark suite  #"
echo "#               v0.1                #"
echo "# # # # # # # # # # # # # # # # # # #"

function create_results_directory {
    mkdir "$RESULTS_DIR"
}

function compile_tests {
    make -C perf
}

function benchmark_download_applications {
    true
}

function benchmark_build_applications {
    true
}

function benchmark_install_applications {
    true
}

function benchmark_test_instrumenting {
    uftrace record --debug-domain dynamic:3 -P. ./applications/cpython/python -c "" 2>&1 \
        | grep -e LATENCY \
            -e "patched:" \
            -e "failed:" \
            -e "skipped:" \
            -e "no match:"
}

function benchmark_test_tracing {
    compile_tests

    # -pg compiled
    uftrace record perf/t-perf-loop-short-pg > "$RESULTS_DIR/tracing-pg.txt"
    # fentry compiled
    uftrace record perf/t-perf-loop-short-fentry > "$RESULTS_DIR/tracing-fentry.txt"
    # dynamic patching
    uftrace -P. record perf/t-perf-loop-short-dynamic > "$RESULTS_DIR/tracing-dynamic.txt"
}

function benchmark_display_results {
    dir=$1

    ./display_results.py "$dir"
}

TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
RESULTS_DIR="results-$TIMESTAMP"

# options
# --output/-O output dir
# --log log uftrace output
# --success-rate --instrumentation-time --overhead test selection (success rate, overhead, ...)
# --uftrace uftrace location
# download/build test applications (choose flags -g -Ox -pg)

# behaviour

getopt --test > /dev/null && exit 1
if [[ $? -ne 4 ]]
then
    echo "getopt (enhanced) not available"
    exit 1
fi

OPTIONS=O:
LONGOPTIONS=results-dir:,output:,log,success-rate,instrumentation-time,overhead,uftrace:,app-binary-size:,app-function-count:,app-language:

PARSED=$(getopt --options=$OPTIONS --longoptions=$LONGOPTIONS --name "$0" -- "$@")
if [[ $? -ne 0 ]]
then
    echo "getopt failed"
    exit 2
fi

eval set -- "$PARSED"

O=results LOGP=n FULLTESTP=y SUCCESSRATEP=n INSTRUMENTATIONTIMEP=n OVERHEADP=n
UFTRACE=$(which uftrace)
while true
do
    case $1 in
        -O|--output)
            O=$2
            shift 2
            ;;
        --log)
            LOGP=y
            shift
            ;;
        --success-rate)
            FULLTESTP=n
            SUCCESSRATEP=y
            shift
            ;;
        --instrumentation-time)
            FULLTESTP=n
            INSTRUMENTATIONTIMEP=y
            shift
            ;;
        --overhead)
            FULLTESTP=n
            OVERHEADP=y
            shift
            ;;
        --results-dir)
            RESULTS_DIR=$2
            shift 2
            ;;
        --)
            shift
            break
            ;;
        *)
            echo "error parsing options"
            exit 3
    esac
done

# check tests to run

if [[ $# -ge 1 ]]
then
    case $1 in
        download)
            benchmark_download_applications
            ;;
        build)
            benchmark_build_applications
            ;;
        install)
            benchmark_install_applications
            ;;
        test-instrumenting)
            create_results_directory
            benchmark_test_instrumenting
            ;;
        test-tracing)
            create_results_directory
            benchmark_test_tracing
            ;;
        display-results)
            benchmark_display_results "$RESULTS_DIR"
            ;;
        *)
            echo "command $1 undefined"
            exit 1
    esac
else
    create_results_directory
    benchmark_test_tracing
    benchmark_display_results "$RESULTS_DIR"
fi
